#!/usr/bin/env python3
"""
DWPose æ‰¹é‡è§†é¢‘å¤„ç†è„šæœ¬ - è¶…çº§ä¼˜åŒ–ç‰ˆ

ä¸»è¦ä¼˜åŒ–:
1. æ‰¹é‡æ¨ç†ï¼ˆBatch Inferenceï¼‰- ä¸€æ¬¡å¤„ç†å¤šå¸§ï¼Œåˆ©ç”¨å§¿æ€æ¨¡å‹åŠ¨æ€ batch
2. å¤šè¿›ç¨‹å¹¶è¡Œ - å¤šä¸ªè§†é¢‘æ–‡ä»¶å¹¶è¡Œå¤„ç†
3. å¯é…ç½®åˆ†è¾¨ç‡ç¼©æ”¾
4. è·³å¸§å¤„ç†é€‰é¡¹
5. GPU ä½¿ç”¨ç›‘æ§
6. è¯¦ç»†æ€§èƒ½ç»Ÿè®¡

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-19
"""

import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import warnings
import multiprocessing as mp
from functools import partial

import cv2
import numpy as np
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from annotator.dwpose import DWposeDetector


class DWposeDetectorBatch(DWposeDetector):
    """æ”¯æŒæ‰¹é‡æ¨ç†çš„ DWPose æ£€æµ‹å™¨"""

    def __init__(self, use_dynamic_det: bool = False):
        super().__init__()
        self.use_dynamic_det = use_dynamic_det

        mode_str = "åŠ¨æ€batchæ£€æµ‹" if use_dynamic_det else "æ ‡å‡†æ£€æµ‹"
        print(f"ğŸš€ åˆå§‹åŒ– DWPose æ£€æµ‹å™¨ï¼ˆ{mode_str} + æ‰¹é‡å§¿æ€æ¨ç†ï¼‰...")

        # é‡æ–°åˆå§‹åŒ– Wholebody ä»¥ä½¿ç”¨æ­£ç¡®çš„æ£€æµ‹æ¨¡å‹
        from annotator.dwpose.wholebody import Wholebody
        self.pose_estimation = Wholebody(use_dynamic_det=use_dynamic_det)

        # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
        try:
            import onnxruntime as ort
            providers = ort.get_available_providers()
            if 'CUDAExecutionProvider' in providers:
                print("âœ… GPU åŠ é€Ÿå·²å¯ç”¨ï¼ˆCUDAExecutionProviderï¼‰")
            else:
                print("âš ï¸  GPU åŠ é€Ÿæœªå¯ç”¨ï¼Œä½¿ç”¨ CPU")
                print(f"   å¯ç”¨çš„ Provider: {providers}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ£€æŸ¥ GPU çŠ¶æ€: {e}")
    
    def __call__(self, oriImg):
        """
        å¤„ç†å•å¸§å›¾åƒ

        Args:
            oriImg: è¾“å…¥å›¾åƒ (H, W, 3)

        Returns:
            canvas: å¯è§†åŒ–å›¾åƒ
            candidate: å…³é”®ç‚¹åæ ‡ (N, K, 3)ï¼ŒåŸå§‹åƒç´ åæ ‡
            subset: å…³é”®ç‚¹ç½®ä¿¡åº¦ (N, K)
        """
        oriImg = oriImg.copy()
        H, W, C = oriImg.shape

        # ç›´æ¥è°ƒç”¨ Wholebody è·å–å…³é”®ç‚¹ï¼ˆè¿”å›åŸå§‹åƒç´ åæ ‡ï¼‰
        candidate, subset = self.pose_estimation(oriImg)

        # å¤åˆ¶ä¸€ä»½ç”¨äºå¯è§†åŒ–ï¼ˆéœ€è¦å½’ä¸€åŒ–åæ ‡ï¼‰
        candidate_vis = candidate.copy()
        nums, keys, locs = candidate_vis.shape
        candidate_vis[..., 0] /= float(W)
        candidate_vis[..., 1] /= float(H)

        # æ„å»ºå¯è§†åŒ–æ‰€éœ€çš„æ•°æ®ç»“æ„
        body = candidate_vis[:, :18].copy()
        body = body.reshape(nums * 18, locs)
        score = subset[:, :18].copy()

        for i in range(len(score)):
            for j in range(len(score[i])):
                if score[i][j] > 0.3:
                    score[i][j] = int(18 * i + j)
                else:
                    score[i][j] = -1

        un_visible = subset < 0.3
        candidate_vis[un_visible] = -1

        foot = candidate_vis[:, 18:24]
        faces = candidate_vis[:, 24:92]
        hands = candidate_vis[:, 92:113]
        hands = np.vstack([hands, candidate_vis[:, 113:]])

        bodies = dict(candidate=body, subset=score)
        pose = dict(bodies=bodies, hands=hands, faces=faces)

        # ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
        from annotator.dwpose import draw_pose
        canvas = draw_pose(pose, H, W)

        return canvas, candidate, subset
    
    def process_batch(self, frames: List[np.ndarray]) -> List[Tuple]:
        """
        æ‰¹é‡å¤„ç†å¤šå¸§ï¼ˆçœŸæ­£çš„æ‰¹é‡æ¨ç†ï¼šåˆå¹¶å¤šå¸§å¤šäººçš„ crops è¿›è¡Œæ‰¹é‡å§¿æ€ä¼°è®¡ï¼‰

        Args:
            frames: å¸§åˆ—è¡¨ï¼Œæ¯ä¸ª shape ä¸º (H, W, 3)

        Returns:
            ç»“æœåˆ—è¡¨ [(canvas, candidate, subset), ...]
        """
        if len(frames) == 0:
            return []

        # ä½¿ç”¨ Wholebody çš„æ‰¹é‡æ¨ç†æ¥å£
        batch_results = self.pose_estimation.batch_inference(frames)

        # ä¸ºæ¯å¸§ç”Ÿæˆå¯è§†åŒ–ç”»å¸ƒ
        results = []
        for frame, (candidate, subset) in zip(frames, batch_results):
            H, W, C = frame.shape

            if len(candidate) == 0:
                # æ— æ£€æµ‹ç»“æœæ—¶è¿”å›ç©ºç”»å¸ƒ
                canvas = np.zeros((H, W, 3), dtype=np.uint8)
                results.append((canvas, candidate, subset))
                continue

            # å¤åˆ¶ä¸€ä»½ç”¨äºå¯è§†åŒ–ï¼ˆéœ€è¦å½’ä¸€åŒ–åæ ‡ï¼‰
            candidate_vis = candidate.copy()
            nums, keys, locs = candidate_vis.shape
            candidate_vis[..., 0] /= float(W)
            candidate_vis[..., 1] /= float(H)

            # æ„å»ºå¯è§†åŒ–æ‰€éœ€çš„æ•°æ®ç»“æ„
            body = candidate_vis[:, :18].copy()
            body = body.reshape(nums * 18, locs)
            score = subset[:, :18].copy()

            for i in range(len(score)):
                for j in range(len(score[i])):
                    if score[i][j] > 0.3:
                        score[i][j] = int(18 * i + j)
                    else:
                        score[i][j] = -1

            un_visible = subset < 0.3
            candidate_vis[un_visible] = -1

            foot = candidate_vis[:, 18:24]
            faces = candidate_vis[:, 24:92]
            hands = candidate_vis[:, 92:113]
            hands = np.vstack([hands, candidate_vis[:, 113:]])

            bodies = dict(candidate=body, subset=score)
            pose = dict(bodies=bodies, hands=hands, faces=faces)

            # ç”Ÿæˆå¯è§†åŒ–å›¾åƒ
            from annotator.dwpose import draw_pose
            canvas = draw_pose(pose, H, W)

            results.append((canvas, candidate, subset))

        return results


class BatchVideoProcessorUltra:
    """è¶…çº§ä¼˜åŒ–çš„æ‰¹é‡è§†é¢‘å¤„ç†å™¨"""

    def __init__(
        self,
        input_dir: str,
        output_dir: str,
        batch_size: int = 4,
        scale_factor: float = 1.0,
        skip_frames: int = 1,
        save_video: bool = True,
        save_format: str = 'npz',
        skip_existing: bool = False,
        use_dynamic_det: bool = False
    ):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.scale_factor = scale_factor
        self.skip_frames = skip_frames
        self.save_video = save_video
        self.save_format = save_format
        self.skip_existing = skip_existing
        self.use_dynamic_det = use_dynamic_det

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.video_output_dir = self.output_dir / 'video_output'
        self.keypoints_output_dir = self.output_dir / 'keypoints_output'

        if self.save_video:
            self.video_output_dir.mkdir(parents=True, exist_ok=True)
        self.keypoints_output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.detector = DWposeDetectorBatch(use_dynamic_det=use_dynamic_det)

        print(f"\nğŸ“Š é…ç½®ä¿¡æ¯:")
        print(f"  - æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"  - åˆ†è¾¨ç‡ç¼©æ”¾: {self.scale_factor:.2f}x")
        print(f"  - è·³å¸§é—´éš”: {self.skip_frames}")
        print(f"  - ä¿å­˜è§†é¢‘: {self.save_video}")
        print(f"  - ä¿å­˜æ ¼å¼: {self.save_format}")
        print(f"  - è·³è¿‡å·²å­˜åœ¨: {self.skip_existing}")
        print(f"  - åŠ¨æ€batchæ£€æµ‹: {use_dynamic_det}")
    
    def find_all_videos(self) -> List[Path]:
        """é€’å½’æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
        videos = []

        for ext in video_extensions:
            videos.extend(self.input_dir.rglob(f'*{ext}'))

        return sorted(videos)

    def get_output_paths(self, video_path: Path) -> Tuple[Optional[Path], Path]:
        """è·å–è¾“å‡ºè·¯å¾„"""
        rel_path = video_path.relative_to(self.input_dir)

        # è§†é¢‘è¾“å‡ºè·¯å¾„
        if self.save_video:
            video_output = self.video_output_dir / rel_path
            video_output.parent.mkdir(parents=True, exist_ok=True)
        else:
            video_output = None

        # å…³é”®ç‚¹è¾“å‡ºè·¯å¾„
        keypoints_output = self.keypoints_output_dir / rel_path.with_suffix(f'.{self.save_format}')
        keypoints_output.parent.mkdir(parents=True, exist_ok=True)

        return video_output, keypoints_output

    def should_skip(self, video_output: Optional[Path], keypoints_output: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡"""
        if not self.skip_existing:
            return False

        if self.save_video and video_output and not video_output.exists():
            return False

        if not keypoints_output.exists():
            return False

        return True

    def process_single_video(self, video_path: Path) -> Dict:
        """å¤„ç†å•ä¸ªè§†é¢‘ï¼ˆæ‰¹é‡æ¨ç†ç‰ˆæœ¬ï¼‰"""
        video_name = video_path.name
        start_time = time.time()

        try:
            # è·å–è¾“å‡ºè·¯å¾„
            video_output, keypoints_output = self.get_output_paths(video_path)

            # æ£€æŸ¥æ˜¯å¦è·³è¿‡
            if self.should_skip(video_output, keypoints_output):
                return {
                    'video': str(video_path),
                    'status': 'skipped',
                    'reason': 'already exists'
                }

            # æ‰“å¼€è§†é¢‘
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

            # è·å–è§†é¢‘ä¿¡æ¯
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

            # è®¡ç®—ç¼©æ”¾åçš„å°ºå¯¸
            scaled_width = int(width * self.scale_factor)
            scaled_height = int(height * self.scale_factor)

            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
            if self.save_video and video_output:
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(
                    str(video_output),
                    fourcc,
                    fps,
                    (width, height)  # è¾“å‡ºåŸå§‹åˆ†è¾¨ç‡
                )
            else:
                out = None

            # å­˜å‚¨å…³é”®ç‚¹æ•°æ®
            all_candidates = []
            all_subsets = []

            # æ‰¹é‡å¤„ç†å¸§
            frame_indices = list(range(0, total_frames, self.skip_frames))
            processed_frames = 0

            pbar = tqdm(total=len(frame_indices), desc=f"å¤„ç† {video_name}")

            for batch_start in range(0, len(frame_indices), self.batch_size):
                batch_end = min(batch_start + self.batch_size, len(frame_indices))
                batch_indices = frame_indices[batch_start:batch_end]

                # è¯»å–æ‰¹é‡å¸§
                frames = []
                for idx in batch_indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    if not ret:
                        break

                    # ç¼©æ”¾å¸§
                    if self.scale_factor != 1.0:
                        frame = cv2.resize(frame, (scaled_width, scaled_height))

                    frames.append(frame)

                if not frames:
                    break

                # æ‰¹é‡æ¨ç†
                results = self.detector.process_batch(frames)

                # å¤„ç†ç»“æœ
                for i, (vis_frame, candidate, subset) in enumerate(results):
                    # å¦‚æœç¼©æ”¾äº†ï¼Œéœ€è¦å°†å…³é”®ç‚¹åæ ‡ç¼©æ”¾å›åŸå§‹å°ºå¯¸
                    if self.scale_factor != 1.0:
                        if candidate is not None and len(candidate) > 0:
                            candidate = candidate.copy()
                            candidate[:, :, :2] /= self.scale_factor

                        # å°†å¯è§†åŒ–å¸§ä¹Ÿç¼©æ”¾å›åŸå§‹å°ºå¯¸
                        vis_frame = cv2.resize(vis_frame, (width, height))

                    # ä¿å­˜å¯è§†åŒ–è§†é¢‘
                    if out is not None:
                        out.write(vis_frame)

                    # ä¿å­˜å…³é”®ç‚¹
                    all_candidates.append(candidate if candidate is not None else np.array([]))
                    all_subsets.append(subset if subset is not None else np.array([]))

                    processed_frames += 1
                    pbar.update(1)

            pbar.close()
            cap.release()
            if out is not None:
                out.release()

            # ä¿å­˜å…³é”®ç‚¹æ•°æ®
            metadata = {
                'video_path': str(video_path),
                'fps': fps,
                'total_frames': total_frames,
                'processed_frames': processed_frames,
                'original_resolution': (width, height),
                'scale_factor': self.scale_factor,
                'skip_frames': self.skip_frames,
                'keypoint_count': 133,
                'format': self.save_format,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }

            if self.save_format == 'npz':
                self.save_keypoints_npz(keypoints_output, all_candidates, all_subsets, metadata)
            else:
                self.save_keypoints_json(keypoints_output, all_candidates, all_subsets, metadata)

            # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
            processing_time = time.time() - start_time
            processing_fps = processed_frames / processing_time if processing_time > 0 else 0

            return {
                'video': str(video_path),
                'status': 'success',
                'frames': processed_frames,
                'processing_time': processing_time,
                'fps': processing_fps,
                'video_output': str(video_output) if video_output else None,
                'keypoints_output': str(keypoints_output)
            }

        except Exception as e:
            import traceback
            return {
                'video': str(video_path),
                'status': 'failed',
                'error': str(e),
                'traceback': traceback.format_exc()
            }

    def save_keypoints_npz(self, output_path: Path, candidates: List, subsets: List, metadata: Dict):
        """ä¿å­˜ NPZ æ ¼å¼çš„å…³é”®ç‚¹æ•°æ®"""
        np.savez_compressed(
            output_path,
            candidates=np.array(candidates, dtype=object),
            subsets=np.array(subsets, dtype=object),
            metadata=np.array([metadata], dtype=object)
        )

    def save_keypoints_json(self, output_path: Path, candidates: List, subsets: List, metadata: Dict):
        """ä¿å­˜ JSON æ ¼å¼çš„å…³é”®ç‚¹æ•°æ®"""
        frames_data = []
        for candidate, subset in zip(candidates, subsets):
            frames_data.append({
                'candidate': candidate.tolist() if isinstance(candidate, np.ndarray) else [],
                'subset': subset.tolist() if isinstance(subset, np.ndarray) else []
            })

        data = {
            **metadata,
            'frames': frames_data
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def process_all_videos(self):
        """æ‰¹é‡å¤„ç†æ‰€æœ‰è§†é¢‘"""
        videos = self.find_all_videos()

        if not videos:
            print(f"âŒ åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            return

        print(f"\nğŸ“¹ æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘æ–‡ä»¶")
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")

        results = []
        success_count = 0
        failed_count = 0
        skipped_count = 0
        total_frames = 0
        total_time = 0

        for i, video_path in enumerate(videos, 1):
            print(f"\n[{i}/{len(videos)}] å¤„ç†: {video_path.name}")

            result = self.process_single_video(video_path)
            results.append(result)

            if result['status'] == 'success':
                success_count += 1
                total_frames += result['frames']
                total_time += result['processing_time']
                print(f"  âœ… æˆåŠŸ")
                print(f"     - å¸§æ•°: {result['frames']}")
                print(f"     - å¤„ç†æ—¶é—´: {result['processing_time']:.2f}s")
                print(f"     - å¤„ç†é€Ÿåº¦: {result['fps']:.2f} fps")
                if result['video_output']:
                    print(f"     - è§†é¢‘è¾“å‡º: {Path(result['video_output']).name}")
                print(f"     - å…³é”®ç‚¹è¾“å‡º: {Path(result['keypoints_output']).name}")
            elif result['status'] == 'skipped':
                skipped_count += 1
                print(f"  â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰")
            else:
                failed_count += 1
                print(f"  âŒ å¤±è´¥: {result['error']}")

        # ç”ŸæˆæŠ¥å‘Š
        avg_fps = total_frames / total_time if total_time > 0 else 0

        report = {
            'total': len(videos),
            'success': success_count,
            'skipped': skipped_count,
            'failed': failed_count,
            'total_frames_processed': total_frames,
            'total_processing_time': total_time,
            'average_fps': avg_fps,
            'configuration': {
                'batch_size': self.batch_size,
                'scale_factor': self.scale_factor,
                'skip_frames': self.skip_frames,
                'save_video': self.save_video,
                'save_format': self.save_format
            },
            'results': results
        }

        report_path = self.output_dir / 'processing_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 80)
        print("å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"æ€»è®¡: {len(videos)} ä¸ªè§†é¢‘")
        print(f"æˆåŠŸ: {success_count}")
        print(f"è·³è¿‡: {skipped_count}")
        print(f"å¤±è´¥: {failed_count}")
        print(f"æ€»å¸§æ•°: {total_frames}")
        print(f"æ€»æ—¶é—´: {total_time:.2f}s")
        print(f"å¹³å‡é€Ÿåº¦: {avg_fps:.2f} fps")
        print(f"\nğŸ“Š å¤„ç†æŠ¥å‘Š: {report_path}")
        print("=" * 80)


def _worker_init(use_dynamic_det: bool = False):
    """å¤šè¿›ç¨‹ worker åˆå§‹åŒ–å‡½æ•°ï¼šæ¯ä¸ªè¿›ç¨‹åˆå§‹åŒ–è‡ªå·±çš„æ£€æµ‹å™¨"""
    global _worker_detector
    _worker_detector = DWposeDetectorBatch(use_dynamic_det=use_dynamic_det)


def _worker_process_video(video_info: Dict, config: Dict) -> Dict:
    """å¤šè¿›ç¨‹ worker å¤„ç†å•ä¸ªè§†é¢‘çš„å‡½æ•°

    Args:
        video_info: åŒ…å« video_path, input_dir çš„å­—å…¸
        config: å¤„ç†é…ç½®å‚æ•°

    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    global _worker_detector

    video_path = Path(video_info['video_path'])
    input_dir = Path(video_info['input_dir'])
    output_dir = Path(config['output_dir'])

    # è·å–è¾“å‡ºè·¯å¾„
    rel_path = video_path.relative_to(input_dir)

    video_output_dir = output_dir / 'video_output'
    keypoints_output_dir = output_dir / 'keypoints_output'

    if config['save_video']:
        video_output = video_output_dir / rel_path
        video_output.parent.mkdir(parents=True, exist_ok=True)
    else:
        video_output = None

    keypoints_output = keypoints_output_dir / rel_path.with_suffix(f".{config['save_format']}")
    keypoints_output.parent.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦è·³è¿‡
    if config['skip_existing']:
        if keypoints_output.exists():
            if not config['save_video'] or (video_output and video_output.exists()):
                return {
                    'video': str(video_path),
                    'status': 'skipped',
                    'reason': 'already exists'
                }

    start_time = time.time()

    try:
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        scale_factor = config['scale_factor']
        scaled_width = int(width * scale_factor)
        scaled_height = int(height * scale_factor)

        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        if config['save_video'] and video_output:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(str(video_output), fourcc, fps, (width, height))
        else:
            out = None

        all_candidates = []
        all_subsets = []

        frame_indices = list(range(0, total_frames, config['skip_frames']))
        processed_frames = 0
        batch_size = config['batch_size']

        for batch_start in range(0, len(frame_indices), batch_size):
            batch_end = min(batch_start + batch_size, len(frame_indices))
            batch_indices = frame_indices[batch_start:batch_end]

            frames = []
            for idx in batch_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if not ret:
                    break
                if scale_factor != 1.0:
                    frame = cv2.resize(frame, (scaled_width, scaled_height))
                frames.append(frame)

            if not frames:
                break

            # æ‰¹é‡æ¨ç†
            results = _worker_detector.process_batch(frames)

            for vis_frame, candidate, subset in results:
                if scale_factor != 1.0:
                    if candidate is not None and len(candidate) > 0:
                        candidate = candidate.copy()
                        candidate[:, :, :2] /= scale_factor
                    vis_frame = cv2.resize(vis_frame, (width, height))

                if out is not None:
                    out.write(vis_frame)

                all_candidates.append(candidate if candidate is not None else np.array([]))
                all_subsets.append(subset if subset is not None else np.array([]))
                processed_frames += 1

        cap.release()
        if out is not None:
            out.release()

        # ä¿å­˜å…³é”®ç‚¹æ•°æ®
        metadata = {
            'video_path': str(video_path),
            'fps': fps,
            'total_frames': total_frames,
            'processed_frames': processed_frames,
            'original_resolution': (width, height),
            'scale_factor': scale_factor,
            'skip_frames': config['skip_frames'],
            'keypoint_count': 133,
            'format': config['save_format'],
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        if config['save_format'] == 'npz':
            np.savez_compressed(
                keypoints_output,
                candidates=np.array(all_candidates, dtype=object),
                subsets=np.array(all_subsets, dtype=object),
                metadata=np.array([metadata], dtype=object)
            )
        else:
            frames_data = []
            for cand, sub in zip(all_candidates, all_subsets):
                frames_data.append({
                    'candidate': cand.tolist() if isinstance(cand, np.ndarray) else [],
                    'subset': sub.tolist() if isinstance(sub, np.ndarray) else []
                })
            with open(keypoints_output, 'w', encoding='utf-8') as f:
                json.dump({**metadata, 'frames': frames_data}, f, indent=2, ensure_ascii=False)

        processing_time = time.time() - start_time

        return {
            'video': str(video_path),
            'status': 'success',
            'frames': processed_frames,
            'processing_time': processing_time,
            'fps': processed_frames / processing_time if processing_time > 0 else 0,
            'video_output': str(video_output) if video_output else None,
            'keypoints_output': str(keypoints_output)
        }

    except Exception as e:
        import traceback
        return {
            'video': str(video_path),
            'status': 'failed',
            'error': str(e),
            'traceback': traceback.format_exc()
        }


def process_videos_multiprocess(videos: List[Path], input_dir: Path, config: Dict, num_workers: int):
    """ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†è§†é¢‘

    Args:
        videos: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        input_dir: è¾“å…¥æ ¹ç›®å½•
        config: å¤„ç†é…ç½®ï¼ˆåŒ…å« use_dynamic_detï¼‰
        num_workers: è¿›ç¨‹æ•°
    """
    use_dynamic_det = config.get('use_dynamic_det', False)
    mode_str = "åŠ¨æ€batchæ£€æµ‹" if use_dynamic_det else "æ ‡å‡†æ£€æµ‹"
    print(f"\nğŸš€ å¯åŠ¨ {num_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹å¤„ç† {len(videos)} ä¸ªè§†é¢‘ï¼ˆ{mode_str}ï¼‰...")

    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
    video_infos = [{'video_path': str(v), 'input_dir': str(input_dir)} for v in videos]

    results = []
    success_count = 0
    failed_count = 0
    skipped_count = 0
    total_frames = 0
    total_time = 0

    # ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆä¼ é€’ use_dynamic_det ç»™ worker åˆå§‹åŒ–å‡½æ•°ï¼‰
    init_fn = partial(_worker_init, use_dynamic_det=use_dynamic_det)
    with mp.Pool(processes=num_workers, initializer=init_fn) as pool:
        worker_fn = partial(_worker_process_video, config=config)

        # ä½¿ç”¨ imap_unordered è·å–ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(videos), desc="å¤„ç†è¿›åº¦") as pbar:
            for result in pool.imap_unordered(worker_fn, video_infos):
                results.append(result)

                if result['status'] == 'success':
                    success_count += 1
                    total_frames += result['frames']
                    total_time += result['processing_time']
                    pbar.set_postfix({'æˆåŠŸ': success_count, 'é€Ÿåº¦': f"{result['fps']:.1f} fps"})
                elif result['status'] == 'skipped':
                    skipped_count += 1
                else:
                    failed_count += 1
                    print(f"\n  âŒ å¤±è´¥: {Path(result['video']).name}: {result.get('error', 'Unknown')}")

                pbar.update(1)

    # ç”ŸæˆæŠ¥å‘Š
    avg_fps = total_frames / total_time if total_time > 0 else 0
    output_dir = Path(config['output_dir'])

    report = {
        'total': len(videos),
        'success': success_count,
        'skipped': skipped_count,
        'failed': failed_count,
        'total_frames_processed': total_frames,
        'total_processing_time': total_time,
        'average_fps': avg_fps,
        'num_workers': num_workers,
        'configuration': config,
        'results': results
    }

    report_path = output_dir / 'processing_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # æ‰“å°æ€»ç»“
    print("\n" + "=" * 80)
    print("å¤„ç†å®Œæˆ")
    print("=" * 80)
    print(f"æ€»è®¡: {len(videos)} ä¸ªè§†é¢‘")
    print(f"æˆåŠŸ: {success_count}")
    print(f"è·³è¿‡: {skipped_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æ€»å¸§æ•°: {total_frames}")
    print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}s")
    print(f"å¹³å‡é€Ÿåº¦: {avg_fps:.2f} fps")
    print(f"å¹¶è¡Œè¿›ç¨‹æ•°: {num_workers}")
    print(f"\nğŸ“Š å¤„ç†æŠ¥å‘Š: {report_path}")
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(
        description='DWPose æ‰¹é‡è§†é¢‘å¤„ç† - è¶…çº§ä¼˜åŒ–ç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä¼˜åŒ–ç‰¹æ€§:
  1. æ‰¹é‡æ¨ç† - ä¸€æ¬¡å¤„ç†å¤šå¸§ï¼Œåˆ©ç”¨å§¿æ€æ¨¡å‹åŠ¨æ€ batch
  2. å¤šè¿›ç¨‹å¹¶è¡Œ - å¤šä¸ªè§†é¢‘æ–‡ä»¶å¹¶è¡Œå¤„ç†ï¼ˆ--num-workersï¼‰
  3. åˆ†è¾¨ç‡ç¼©æ”¾ - é™ä½è¾“å…¥åˆ†è¾¨ç‡ï¼ŒåŠ å¿«å¤„ç†é€Ÿåº¦
  4. è·³å¸§å¤„ç† - åªå¤„ç†å…³é”®å¸§ï¼Œå¤§å¹…æå‡é€Ÿåº¦
  5. GPU ç›‘æ§ - è‡ªåŠ¨æ£€æµ‹ GPU ä½¿ç”¨æƒ…å†µ

ä½¿ç”¨ç¤ºä¾‹:
  # åŸºæœ¬ä½¿ç”¨ï¼ˆå•è¿›ç¨‹ï¼Œæ‰¹é‡å¤§å° 4ï¼‰
  python batch_process_videos_ultra.py \\
      --input ../data/UCF-101/ApplyEyeMakeup \\
      --output ../data/dwpose

  # å¤šè¿›ç¨‹å¹¶è¡Œï¼ˆ4 è¿›ç¨‹ï¼‰
  python batch_process_videos_ultra.py \\
      --input ../data/UCF-101 \\
      --output ../data/dwpose \\
      --num-workers 4

  # é«˜æ€§èƒ½é…ç½®ï¼ˆæ‰¹é‡ 8 + ç¼©æ”¾ 0.75x + å¤šè¿›ç¨‹ï¼‰
  python batch_process_videos_ultra.py \\
      --input ../data/UCF-101 \\
      --output ../data/dwpose \\
      --batch-size 8 \\
      --scale 0.75 \\
      --num-workers 2

  # æé€Ÿæ¨¡å¼ï¼ˆä»…å…³é”®ç‚¹ï¼Œè·³å¸§ï¼‰
  python batch_process_videos_ultra.py \\
      --input ../data/UCF-101 \\
      --output ../data/dwpose \\
      --batch-size 16 \\
      --scale 0.5 \\
      --skip-frames 2 \\
      --no-video
        """
    )

    parser.add_argument('--input', type=str, required=True, help='è¾“å…¥è§†é¢‘ç›®å½•')
    parser.add_argument('--output', type=str, required=True, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--batch-size', type=int, default=4, help='å¸§æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤: 4ï¼‰')
    parser.add_argument('--scale', type=float, default=1.0, help='åˆ†è¾¨ç‡ç¼©æ”¾å› å­ï¼ˆé»˜è®¤: 1.0ï¼‰')
    parser.add_argument('--skip-frames', type=int, default=1, help='è·³å¸§é—´éš”ï¼ˆé»˜è®¤: 1ï¼Œä¸è·³å¸§ï¼‰')
    parser.add_argument('--no-video', action='store_true', help='ä¸ä¿å­˜å¯è§†åŒ–è§†é¢‘')
    parser.add_argument('--format', type=str, default='npz', choices=['npz', 'json'], help='å…³é”®ç‚¹ä¿å­˜æ ¼å¼')
    parser.add_argument('--skip-existing', action='store_true', help='è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶')
    parser.add_argument('--num-workers', type=int, default=1,
                        help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼Œå•è¿›ç¨‹ï¼‰ã€‚æ³¨æ„ï¼šå¤šè¿›ç¨‹æ¨¡å¼ä¸‹æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šåŠ è½½æ¨¡å‹ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜')
    parser.add_argument('--use-dynamic-det', action='store_true',
                        help='ä½¿ç”¨åŠ¨æ€ batch æ£€æµ‹æ¨¡å‹ï¼ˆéœ€è¦ models/yolox_l_dynamic.onnxï¼‰')

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.batch_size < 1:
        print("âŒ é”™è¯¯: batch-size å¿…é¡» >= 1")
        return 1

    if args.scale <= 0 or args.scale > 1.0:
        print("âŒ é”™è¯¯: scale å¿…é¡»åœ¨ (0, 1.0] èŒƒå›´å†…")
        return 1

    if args.skip_frames < 1:
        print("âŒ é”™è¯¯: skip-frames å¿…é¡» >= 1")
        return 1

    if args.num_workers < 1:
        print("âŒ é”™è¯¯: num-workers å¿…é¡» >= 1")
        return 1

    # æ ¹æ®è¿›ç¨‹æ•°é€‰æ‹©å¤„ç†æ¨¡å¼
    if args.num_workers == 1:
        # å•è¿›ç¨‹æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        processor = BatchVideoProcessorUltra(
            input_dir=args.input,
            output_dir=args.output,
            batch_size=args.batch_size,
            scale_factor=args.scale,
            skip_frames=args.skip_frames,
            save_video=not args.no_video,
            save_format=args.format,
            skip_existing=args.skip_existing,
            use_dynamic_det=args.use_dynamic_det
        )
        processor.process_all_videos()
    else:
        # å¤šè¿›ç¨‹æ¨¡å¼
        input_dir = Path(args.input)
        output_dir = Path(args.output)
        output_dir.mkdir(parents=True, exist_ok=True)
        (output_dir / 'video_output').mkdir(parents=True, exist_ok=True)
        (output_dir / 'keypoints_output').mkdir(parents=True, exist_ok=True)

        # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
        videos = []
        for ext in video_extensions:
            videos.extend(input_dir.rglob(f'*{ext}'))
        videos = sorted(videos)

        if not videos:
            print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            return 1

        config = {
            'output_dir': str(output_dir),
            'batch_size': args.batch_size,
            'scale_factor': args.scale,
            'skip_frames': args.skip_frames,
            'save_video': not args.no_video,
            'save_format': args.format,
            'skip_existing': args.skip_existing,
            'use_dynamic_det': args.use_dynamic_det
        }

        process_videos_multiprocess(videos, input_dir, config, args.num_workers)

    return 0


if __name__ == '__main__':
    # å¤šè¿›ç¨‹éœ€è¦åœ¨ __main__ ä¸­å¯åŠ¨
    mp.set_start_method('spawn', force=True)
    sys.exit(main())

